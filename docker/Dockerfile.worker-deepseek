# ============================================
# D8 Worker Node - DeepSeek Optimized
# ============================================
# Specifically for Raspberry Pi 4 with DeepSeek inference
# Includes Ollama for local LLM serving

FROM python:3.11-slim-bookworm

ARG TARGETPLATFORM
ARG BUILDPLATFORM

ENV PYTHONUNBUFFERED=1
ENV WORKER_TYPE=deepseek
ENV DEBIAN_FRONTEND=noninteractive
ENV OLLAMA_HOST=0.0.0.0:11434

LABEL maintainer="D8 Project"
LABEL description="D8 Worker Node with DeepSeek (Ollama)"
LABEL optimized.for="Raspberry Pi 4"

# Install system dependencies + Ollama requirements
RUN apt-get update && apt-get install -y \
    curl \
    git \
    build-essential \
    libssl-dev \
    libffi-dev \
    ca-certificates \
    && rm -rf /var/lib/apt/lists/*

# Install Ollama (ARM64 compatible)
RUN curl -fsSL https://ollama.com/install.sh | sh

WORKDIR /app

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY lib/ ./lib/
COPY app/ ./app/
COPY scripts/ ./scripts/

# Create necessary directories
RUN mkdir -p /app/data/logs \
    /app/data/cache \
    /app/data/ollama_models \
    /root/.ollama/models

# Copy startup scripts
COPY docker/entrypoint-worker-deepseek.sh /entrypoint.sh
COPY docker/init-ollama.sh /init-ollama.sh
RUN chmod +x /entrypoint.sh /init-ollama.sh

# Health check (check both Ollama and worker)
HEALTHCHECK --interval=60s --timeout=15s --start-period=120s --retries=3 \
    CMD curl -f http://localhost:11434/api/tags && \
        python -c "import requests; requests.get('http://localhost:8080/health', timeout=5)" || exit 1

# Expose ports
EXPOSE 8080 11434

# Entrypoint starts Ollama + worker
ENTRYPOINT ["/entrypoint.sh"]
