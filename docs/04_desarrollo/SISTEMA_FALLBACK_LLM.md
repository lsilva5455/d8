# üõ°Ô∏è Sistema Hiper Robusto de Fallback de LLMs

**Fecha de implementaci√≥n:** 2025-11-21  
**Estado:** ‚úÖ OPERACIONAL  
**Componentes:** 3 archivos nuevos, 2 modificados

---

## üéØ Objetivo

Crear un sistema **completamente aut√≥nomo** que:
1. ‚úÖ **Maneje fallos de IA autom√°ticamente** (rate limit, timeout, etc.)
2. ‚úÖ **Fallback autom√°tico** entre providers (Groq ‚Üí Gemini ‚Üí DeepSeek)
3. ‚úÖ **Derive al Congreso** cuando todo falla
4. ‚úÖ **No requiera intervenci√≥n humana**

---

## üèóÔ∏è Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    BASE AGENT                           ‚îÇ
‚îÇ  (act() method)                                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            LLM FALLBACK MANAGER (Singleton)             ‚îÇ
‚îÇ  - Detecci√≥n inteligente de errores                     ‚îÇ
‚îÇ  - Cooldowns adaptativos                                ‚îÇ
‚îÇ  - Health tracking                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ             ‚îÇ             ‚îÇ
         ‚ñº             ‚ñº             ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  GROQ  ‚îÇ    ‚îÇ GEMINI ‚îÇ    ‚îÇ DEEPSEEK ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ             ‚îÇ             ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                 ¬øTodo fall√≥?
                       ‚îÇ
                       ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  CONGRESO        ‚îÇ
              ‚îÇ  AUT√ìNOMO        ‚îÇ
              ‚îÇ  (Escalaci√≥n)    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üì¶ Componentes Implementados

### 1. **LLMFallbackManager** 
**Archivo:** `lib/llm/fallback_manager.py` (750+ l√≠neas)

**Caracter√≠sticas:**
- ‚úÖ Fallback autom√°tico: Groq ‚Üí Gemini ‚Üí DeepSeek
- ‚úÖ Detecci√≥n inteligente de 6 tipos de errores:
  - `RATE_LIMIT` (429) ‚Üí Cooldown 60s
  - `TIMEOUT` ‚Üí Cooldown 30s
  - `AUTH` ‚Üí Marcar provider no disponible
  - `UNAVAILABLE` (503) ‚Üí Cooldown adaptativo
  - `INVALID_RESPONSE` ‚Üí Cooldown adaptativo
  - `UNKNOWN` ‚Üí Backoff exponencial (5s ‚Üí 80s)
- ‚úÖ Health tracking por provider
- ‚úÖ Persistencia de estado en `~/Documents/d8_data/llm_fallback/fallback_state.json`
- ‚úÖ Historial de errores (√∫ltimos 50)
- ‚úÖ Derivaci√≥n autom√°tica al Congreso

**M√©todos principales:**
```python
llm_manager.chat(
    messages=[...],
    temperature=0.7,
    max_tokens=2000,
    json_mode=True,
    context="Descripci√≥n del contexto"
)
# Returns: (response, provider_used) o (None, "failed")
```

---

### 2. **Singleton Global**
**Archivo:** `app/llm_manager_singleton.py`

**Uso:**
```python
from app.llm_manager_singleton import get_llm_manager

llm_manager = get_llm_manager()
# Instancia √∫nica compartida por todo el sistema
```

---

### 3. **BaseAgent Actualizado**
**Archivo:** `app/agents/base_agent.py`

**Cambios:**
- ‚ùå **Removed:** Dependencia directa de `groq.Groq`
- ‚úÖ **Added:** Usa `LLMFallbackManager` via singleton
- ‚úÖ **Added:** Par√°metro `llm_manager` en `__init__`
- ‚úÖ **Added:** Info de provider usado en respuesta: `result["llm_provider"]`
- ‚úÖ **Added:** Detecci√≥n de escalaci√≥n al Congreso: `result["escalated_to_congress"]`

**Compatibilidad:**
- Par√°metro `groq_api_key` deprecated pero mantenido para no romper c√≥digo existente
- Se ignora si se pasa, el manager usa las keys del `.env`

---

### 4. **Endpoint de Monitoreo**
**Archivo:** `app/orchestrator_app.py`

**Nuevo endpoint:**
```bash
GET http://localhost:7001/api/llm/health
```

**Response:**
```json
{
  "timestamp": "2025-11-21T10:00:00",
  "total_requests": 123,
  "congress_escalations": 5,
  "providers": {
    "groq": {
      "is_available": true,
      "consecutive_failures": 0,
      "total_requests": 100,
      "total_failures": 5,
      "success_rate": 95.0,
      "in_cooldown": false,
      "last_error_type": null
    },
    "gemini": {...},
    "deepseek": {...}
  },
  "recent_errors": [...]
}
```

---

## üöÄ Uso

### Opci√≥n A: Autom√°tico (Recomendado)
```python
from app.agents.base_agent import BaseAgent
from app.evolution.darwin import Genome

# Crear genome
genome = Genome(prompt="You are an AI agent...", generation=1)

# Crear agente (usa LLM Manager autom√°ticamente)
agent = BaseAgent(genome=genome)

# Actuar (fallback autom√°tico incluido)
result = agent.act(
    input_data={"task": "Analyze market trends"},
    action_type="analyze"
)

# Verificar resultado
if result.get("success") == False and result.get("escalated_to_congress"):
    print("üèõÔ∏è Problema derivado al Congreso")
else:
    provider = result.get("llm_provider", "unknown")
    print(f"‚úÖ Completado con {provider}")
```

### Opci√≥n B: Uso Directo del Manager
```python
from app.llm_manager_singleton import get_llm_manager

llm_manager = get_llm_manager()

messages = [
    {"role": "system", "content": "You are..."},
    {"role": "user", "content": "Task..."}
]

response, provider = llm_manager.chat(
    messages=messages,
    context="Descripci√≥n del contexto"
)

if response is None:
    print("‚ùå Todos los providers fallaron")
else:
    print(f"‚úÖ √âxito con {provider}")
```

---

## üß™ Testing

### Script de prueba:
```bash
python scripts/tests/test_llm_fallback.py
```

**Output esperado:**
```
üß™ TEST: Sistema de Fallback Autom√°tico de LLMs
===============================================

üìù Creando agente de prueba...
‚úÖ Agente creado: a3b4c5d6

üìä Estado inicial de providers:
   ‚úÖ GROQ: 0.0% √©xito
   ‚úÖ GEMINI: 0.0% √©xito
   ‚úÖ DEEPSEEK: 0.0% √©xito

TEST 1: Request normal
----------------------------------------------------------------------
‚úÖ Request exitoso usando: GROQ

TEST 2: Segundo request
----------------------------------------------------------------------
‚úÖ Request exitoso usando: GEMINI  # ‚Üê Fallback autom√°tico!

üìä Estado final de providers:
----------------------------------------------------------------------
üìà Total requests: 2
üèõÔ∏è  Escalaciones al Congreso: 0

‚úÖ GROQ ‚è≥ EN COOLDOWN
   Requests: 1
   Fallos: 1
   Tasa de √©xito: 0.0%
   √öltimo error: rate_limit

‚úÖ GEMINI
   Requests: 1
   Fallos: 0
   Tasa de √©xito: 100.0%
```

---

## üèõÔ∏è Derivaci√≥n al Congreso

### ¬øCu√°ndo se deriva?

1. **Todos los providers fallaron** despu√©s de reintentos
2. **Mismo error se repite 5+ veces** (configurable)
3. **10+ fallos totales** en ventana de tiempo

### Archivos generados:

**Directorio:** `~/Documents/d8_data/llm_fallback/`

**Archivos:**
- `congress_escalation_YYYYMMDD_HHMMSS.json` - Detalles de cada escalaci√≥n
- `fallback_state.json` - Estado persistente del sistema

### Formato de escalaci√≥n:
```json
{
  "timestamp": "2025-11-21T10:30:00",
  "escalation_number": 1,
  "context": "Agent abc123 - Action: analyze",
  "messages": [...],
  "provider_status": {
    "groq": {
      "is_available": false,
      "consecutive_failures": 5,
      "last_error": "Rate limit reached",
      "error_type": "rate_limit"
    },
    ...
  },
  "error_history": [...],
  "proposal_description": "## üö® ESCALACI√ìN AUTOM√ÅTICA..."
}
```

### Propuesta al Congreso:

Si el `ProposalSystem` est√° disponible, se crea autom√°ticamente:

```python
Propuesta:
  - T√≠tulo: "üö® Fallo Cr√≠tico LLM - Escalaci√≥n #1"
  - Categor√≠a: TECHNICAL
  - Prioridad: 1 (CR√çTICA)
  - Tags: ["llm", "infrastructure", "auto-escalation"]
  - Metadata: {providers_failed, error_types, ...}
```

---

## üìä Configuraci√≥n

### Variables de entorno (.env):
```bash
# Groq (primario)
GROQ_API_KEY=gsk_xxx

# Gemini (fallback 1)
GEMINI_API_KEY=AIza_xxx

# DeepSeek (fallback 2)
DEEPSEEK_BASE_URL=http://localhost:7100
```

### Configuraci√≥n avanzada:
```python
from lib.llm import FallbackConfig

config = FallbackConfig(
    provider_priority=["groq", "gemini", "deepseek"],
    max_retries_per_provider=2,  # Reintentos antes de cambiar
    congress_threshold_failures=10,  # Fallos para derivar
    congress_threshold_repeated_error=5,  # Mismo error repetido
    enable_congress_escalation=True  # Habilitar derivaci√≥n
)
```

---

## üîç Monitoreo

### Comando r√°pido:
```bash
# Ver estado del LLM Manager
curl http://localhost:7001/api/llm/health | jq

# Ver escalaciones recientes
ls -lh ~/Documents/d8_data/llm_fallback/congress_escalation_*.json

# Ver √∫ltimo estado guardado
cat ~/Documents/d8_data/llm_fallback/fallback_state.json | jq
```

### Dashboard (futuro):
- Acceder a `http://localhost:7001/dashboard/llm` (TODO)

---

## üìà M√©tricas

| M√©trica | Descripci√≥n |
|---------|-------------|
| `total_requests` | Total de requests al sistema |
| `congress_escalations` | Cu√°ntas veces se deriv√≥ al Congreso |
| `success_rate` | % de √©xito por provider |
| `consecutive_failures` | Fallos seguidos (reset al tener √©xito) |
| `in_cooldown` | Si provider est√° en per√≠odo de espera |

---

## üéØ Beneficios

‚úÖ **Resiliencia**: Sistema funciona aunque Groq est√© en rate limit  
‚úÖ **Autonom√≠a**: Cero intervenci√≥n humana, deriva al Congreso  
‚úÖ **Visibilidad**: Tracking de salud de cada provider  
‚úÖ **Inteligencia**: Cooldowns adaptativos seg√∫n tipo de error  
‚úÖ **Persistencia**: Estado guardado entre reinicios  
‚úÖ **Escalabilidad**: F√°cil agregar nuevos providers

---

## üîß Troubleshooting

### Problema: "Todos los providers fallaron"
**Soluci√≥n:**
1. Verificar API keys en `.env`
2. Ver endpoint `/api/llm/health` para detalles
3. Revisar archivos de escalaci√≥n en `~/Documents/d8_data/llm_fallback/`
4. Verificar cooldowns (esperar 60s si rate limit)

### Problema: "Escalaciones frecuentes al Congreso"
**Soluci√≥n:**
1. Revisar logs del Congreso en `~/Documents/d8_data/logs/congress.log`
2. Verificar propuestas creadas (deber√≠a haber 1 por escalaci√≥n)
3. Ajustar `congress_threshold_failures` en config

### Problema: "Provider siempre en cooldown"
**Soluci√≥n:**
1. Verificar `consecutive_failures` en health report
2. Si >5, provider se marc√≥ como no disponible
3. Reiniciar sistema para resetear cooldowns
4. Verificar API keys y quotas

---

## üöÄ Pr√≥ximos Pasos

### Fase 1: Completado ‚úÖ
- [x] LLMFallbackManager con detecci√≥n de errores
- [x] Integraci√≥n en BaseAgent
- [x] Derivaci√≥n al Congreso
- [x] Endpoint de monitoreo
- [x] Tests b√°sicos
- [x] Documentaci√≥n

### Fase 2: Mejoras Futuras
- [ ] Dashboard web para visualizar salud
- [ ] Auto-rotate API keys si m√∫ltiples disponibles
- [ ] Cache de respuestas para reducir requests
- [ ] Metrics en Prometheus format
- [ ] Alertas por Telegram cuando hay escalaci√≥n
- [ ] Auto-resoluci√≥n de propuestas del Congreso

---

**√öltima actualizaci√≥n:** 2025-11-21  
**Autor:** GitHub Copilot + Usuario  
**Estado:** ‚úÖ OPERACIONAL - Listo para producci√≥n
